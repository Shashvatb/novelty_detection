{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db883ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import sys\n",
    "import logging\n",
    "import numpy as np\n",
    "import os\n",
    "from utils import dir_hash, parse_json, gen_observations, preprocess\n",
    "from mem_net import run_mem_net, test_mem_network\n",
    "\n",
    "logging.basicConfig(level=logging.DEBUG)\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.addHandler(logging.StreamHandler())\n",
    "logger.setLevel(logging.DEBUG)\n",
    "cache_pickle = \"{}.pkl\"\n",
    "cache_dir = \".cache-pythia\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19b407f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_args(\n",
    "        # DIRECTORY\n",
    "        directory,\n",
    "\n",
    "        # one-hot CNN layer\n",
    "        CNN_APPEND=False,\n",
    "        CNN_DIFFERENCE=False,\n",
    "        CNN_PRODUCT=False,\n",
    "        CNN_COS=False,\n",
    "        # The one-hot CNN will use the full_vocab parameters\n",
    "        # memory network\n",
    "        MEM_VOCAB=0,\n",
    "        MEM_TYPE='dmn_basic',\n",
    "        MEM_BATCH=1,\n",
    "        MEM_EPOCHS=5,\n",
    "\n",
    "        # PARAMETERS\n",
    "        # resampling\n",
    "        RESAMPLING=False,\n",
    "        NOVEL_RATIO=None,\n",
    "        OVERSAMPLING=False,\n",
    "        REPLACEMENT=False,\n",
    "        SAVE_RESULTS=False,\n",
    "\n",
    "        # save training data for experimentation and hyperparameter grid search\n",
    "        SAVEEXPERIMENTDATA=False,\n",
    "        EXPERIMENTDATAFILE='data/experimentdatafile.pkl',\n",
    "\n",
    "        # vocabulary\n",
    "        VOCAB_SIZE=10000,\n",
    "        STEM=False,\n",
    "        FULL_VOCAB_SIZE=10000,\n",
    "        FULL_VOCAB_TYPE='character',\n",
    "        FULL_CHAR_VOCAB=\"abcdefghijklmnopqrstuvwxyz0123456789,;.!?:'\\\"/|_@#$%^&*~`+-=<>()[]{}\",\n",
    "        FULL_VOCAB_STEM=False,\n",
    "        SEED=41,\n",
    "\n",
    "        USE_CACHE=False):\n",
    "    \"\"\" Return a parameters data structure with information on how to\n",
    "    run an experiment. Argument list should match experiments/experiments.py\n",
    "    \"\"\"\n",
    "\n",
    "    # get features\n",
    "\n",
    "    cnn = dict()\n",
    "    if CNN_APPEND: cnn['append'] = CNN_APPEND\n",
    "    if CNN_DIFFERENCE: cnn['difference'] = CNN_DIFFERENCE\n",
    "    if CNN_PRODUCT: cnn['product'] = CNN_PRODUCT\n",
    "    if CNN_COS: cnn['cos'] = CNN_COS\n",
    "    mem_net = dict()\n",
    "    if MEM_VOCAB: mem_net['word_vector_size'] = MEM_VOCAB\n",
    "    # if SEED: mem_net['seed'] = SEED\n",
    "    if MEM_TYPE: mem_net['network'] = MEM_TYPE\n",
    "    if MEM_BATCH: mem_net['batch_size'] = MEM_BATCH\n",
    "    if MEM_EPOCHS: mem_net['epochs'] = MEM_EPOCHS\n",
    "\n",
    "    # Use the same input params as word2vec\n",
    "\n",
    "    features = dict()\n",
    "    features['cnn'] = cnn\n",
    "    if len(features) > 0:\n",
    "        print(\"Caution!!  Only the memory network feature and algorithm will be ran as they have to run alone\")\n",
    "    features['mem_net'] = mem_net\n",
    "\n",
    "    if len(features) == 0:\n",
    "        raise Exception(\"Error: No features found\")\n",
    "\n",
    "    # get algorithms\n",
    "    algorithms = dict()\n",
    "    algorithms['mem_net'] = mem_net\n",
    "\n",
    "    logger.debug(\"Algorithms structure: {}\".format(algorithms))\n",
    "\n",
    "    # Enforce requirement and limitation of one algorithm per run\n",
    "    if len(algorithms) == 0:\n",
    "        print(\"Error: One classification algorithm must be requested per run.\", file=sys.stderr)\n",
    "        quit()\n",
    "    elif len(algorithms) > 1:\n",
    "        print(\"Error: Only one classification can be requested per run.\", file=sys.stderr)\n",
    "        quit()\n",
    "\n",
    "    # get parameters\n",
    "    resampling = None\n",
    "\n",
    "    if RESAMPLING:\n",
    "        resampling = dict()\n",
    "        if NOVEL_RATIO:\n",
    "            resampling['novelToNotNovelRatio'] = NOVEL_RATIO\n",
    "            logger.warning(\"NOVEL_RATIO specified but not supported\")\n",
    "        resampling['over'] = OVERSAMPLING\n",
    "        resampling['replacement'] = REPLACEMENT\n",
    "\n",
    "    saveexperimentdata = None\n",
    "    if SAVEEXPERIMENTDATA:\n",
    "        saveexperimentdata = dict()\n",
    "        if EXPERIMENTDATAFILE: saveexperimentdata['experimentdatafile'] = EXPERIMENTDATAFILE\n",
    "\n",
    "    parameters = dict()\n",
    "    if RESAMPLING: parameters['resampling'] = resampling\n",
    "    if SAVE_RESULTS: parameters['save_results'] = SAVE_RESULTS\n",
    "    if SAVEEXPERIMENTDATA: parameters['saveexperimentdata'] = saveexperimentdata\n",
    "    if VOCAB_SIZE: parameters['vocab'] = VOCAB_SIZE\n",
    "    if STEM: parameters['stem'] = STEM\n",
    "    if SEED:\n",
    "        parameters['seed'] = SEED\n",
    "    else:\n",
    "        parameters['seed'] = 41\n",
    "    if FULL_VOCAB_SIZE: parameters['full_vocab_size'] = FULL_VOCAB_SIZE\n",
    "    if FULL_VOCAB_TYPE: parameters['full_vocab_type'] = FULL_VOCAB_TYPE\n",
    "    if FULL_CHAR_VOCAB: parameters['full_char_vocab'] = FULL_CHAR_VOCAB\n",
    "    if FULL_VOCAB_STEM: parameters['full_vocab_stem'] = FULL_VOCAB_STEM\n",
    "    parameters['use_cache'] = USE_CACHE\n",
    "\n",
    "    return directory, features, algorithms, parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b4c6f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(argv):\n",
    "    \"\"\"\n",
    "    controls the over-arching implmentation of the algorithms\n",
    "    \"\"\"\n",
    "    print('starting')\n",
    "    directory, features, algorithms, parameters = argv\n",
    "\n",
    "    # Create a numpy random state\n",
    "    random_state = np.random.RandomState(parameters['seed'])\n",
    "\n",
    "    # parsing\n",
    "    print(\"parsing json data...\", file=sys.stderr)\n",
    "\n",
    "    if parameters['use_cache']:\n",
    "        hash = dir_hash(directory)\n",
    "        pickle_path = os.path.join(cache_dir, cache_pickle.format(hash))\n",
    "        try:\n",
    "            logger.debug(\"Trying to use cache\")\n",
    "            with open(pickle_path, 'rb') as f:\n",
    "                parsed_data = pickle.load(f)\n",
    "                logger.debug(\"Using existing cache\")\n",
    "        except:\n",
    "            # parse and write to cache\n",
    "            logger.debug(\"Parsing and writing to cache\")\n",
    "            parsed_data = parse_json(directory, parameters)\n",
    "            os.makedirs(cache_dir, exist_ok=True)\n",
    "            with open(pickle_path, 'wb') as f:\n",
    "                pickle.dump(parsed_data, f)\n",
    "    else:\n",
    "        parsed_data = parse_json(directory, parameters)\n",
    "    clusters, order, data, test_clusters, test_order, test_data, corpusdict = parsed_data\n",
    "\n",
    "    # preprocessing\n",
    "    print(\"preprocessing...\", file=sys.stderr)\n",
    "    vocab, full_vocab, encoder_decoder, lda_model, tf_model, w2v_model = preprocess(features, parameters,\n",
    "                                                                                         corpusdict, data)\n",
    "\n",
    "    # featurization\n",
    "    hdf5_path_train = parameters['hdf5_path_train']\n",
    "    hdf5_path_test = parameters['hdf5_path_test']\n",
    "    print(\"generating training data...\", file=sys.stderr)\n",
    "    train_data, train_target, train_ids = gen_observations(clusters, order, data, features, parameters, vocab,\n",
    "                                                                    full_vocab, encoder_decoder, lda_model, tf_model,\n",
    "                                                                    w2v_model, hdf5_path_train)\n",
    "    print(\"generating testing data...\", file=sys.stderr)\n",
    "    test_data, test_target, test_ids = gen_observations(test_clusters, test_order, test_data, features,\n",
    "                                                                 parameters, vocab, full_vocab, encoder_decoder,\n",
    "                                                                 lda_model, tf_model, w2v_model, hdf5_path_test)\n",
    "\n",
    "    # save training data for separate experimentation and hyperparameter optimization\n",
    "    if 'saveexperimentdata' in parameters:\n",
    "        lunchbox = dict()\n",
    "        lunchbox['directory'] = directory\n",
    "        lunchbox['features'] = features\n",
    "        lunchbox['algorithms'] = algorithms\n",
    "        lunchbox['parameters'] = parameters\n",
    "        lunchbox['train_data'] = train_data\n",
    "        lunchbox['train_target'] = train_target\n",
    "        lunchbox['test_data'] = test_data\n",
    "        lunchbox['test_target'] = test_target\n",
    "        pickle.dump(lunchbox, open(parameters['saveexperimentdata']['experimentdatafile'], \"wb\"))\n",
    "\n",
    "    # modeling\n",
    "    print(\"running algorithms...\", file=sys.stderr)\n",
    "    if 'mem_net' in algorithms:\n",
    "        mem_net_model, model_name = run_mem_net(train_data, test_data, **algorithms['mem_net'])\n",
    "        predicted_labels, perform_results = test_mem_network(mem_net_model, model_name,\n",
    "                                                                          **algorithms['mem_net'])\n",
    "    # results\n",
    "    if \"save_results\" in parameters:\n",
    "        perform_results.update({\"id\": test_ids})\n",
    "        perform_results.update({\"predicted_label\": predicted_labels.tolist()})\n",
    "        perform_results.update({\"novelty\": test_target})\n",
    "\n",
    "    return perform_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ed31a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    args = get_args()\n",
    "    print(\"Algorithm details and Results:\", file=sys.stderr)\n",
    "    print(main(args), file=sys.stdout)\n",
    "    sys.exit(0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
