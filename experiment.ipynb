{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ad653f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import sys\n",
    "import logging\n",
    "import numpy as np\n",
    "import os\n",
    "from utils import dir_hash, parse_json, gen_observations, preprocess\n",
    "from mem_net import run_mem_net, test_mem_network\n",
    "\n",
    "logging.basicConfig(level=logging.DEBUG)\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.addHandler(logging.StreamHandler())\n",
    "logger.setLevel(logging.DEBUG)\n",
    "cache_pickle = \"{}.pkl\"\n",
    "cache_dir = \".cache-pythia\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c445251",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_args(\n",
    "        # DIRECTORY\n",
    "        directory='data/stackexchange/anime',\n",
    "\n",
    "        # FEATURES\n",
    "        # bag of words\n",
    "        BOW_APPEND=False,\n",
    "        BOW_DIFFERENCE=False,\n",
    "        BOW_PRODUCT=False,\n",
    "        BOW_COS=False,\n",
    "        BOW_TFIDF=False,\n",
    "        BOW_BINARY=True,\n",
    "\n",
    "        # skipthoughts\n",
    "        ST_APPEND=False,\n",
    "        ST_DIFFERENCE=False,\n",
    "        ST_PRODUCT=False,\n",
    "        ST_COS=False,\n",
    "\n",
    "        # lda\n",
    "        LDA_APPEND=False,\n",
    "        LDA_DIFFERENCE=False,\n",
    "        LDA_PRODUCT=False,\n",
    "        LDA_COS=False,\n",
    "        LDA_TOPICS=40,\n",
    "\n",
    "        # word2vec\n",
    "        # If AVG, MAX, MIN or ABS are selected, APPEND, DIFFERENCE, PRODUCT or COS must be selected\n",
    "        W2V_AVG=False,\n",
    "        W2V_MAX=False,\n",
    "        W2V_MIN=False,\n",
    "        W2V_ABS=False,\n",
    "        # If APPEND, DIFFERENCE, PRODUCT or COS are selected AVG, MAX, MIN or ABS must be selected\n",
    "        W2V_APPEND=False,\n",
    "        W2V_DIFFERENCE=False,\n",
    "        W2V_PRODUCT=False,\n",
    "        W2V_COS=False,\n",
    "        W2V_PRETRAINED=False,\n",
    "        W2V_MIN_COUNT=5,\n",
    "        W2V_WINDOW=5,\n",
    "        # W2V_SIZE should be set to 300 if using the Google News pretrained word2vec model\n",
    "        W2V_SIZE=300,\n",
    "        W2V_WORKERS=3,\n",
    "\n",
    "        # one-hot CNN layer\n",
    "        CNN_APPEND=False,\n",
    "        CNN_DIFFERENCE=False,\n",
    "        CNN_PRODUCT=False,\n",
    "        CNN_COS=False,\n",
    "        # The one-hot CNN will use the full_vocab parameters\n",
    "\n",
    "        # wordonehot (will not play nicely with other featurization methods b/c not\n",
    "        # vector)\n",
    "        WORDONEHOT=False,\n",
    "        # WORDONEHOT_DOCLENGTH = None\n",
    "        WORDONEHOT_VOCAB=5000,\n",
    "\n",
    "        # ALGORITHMS\n",
    "        # logistic regression\n",
    "        LOG_REG=False,\n",
    "        LOG_PENALTY='l2',\n",
    "        LOG_TOL=1e-4,\n",
    "        LOG_C=1e-4,\n",
    "\n",
    "        # svm\n",
    "        SVM=False,\n",
    "        SVM_C=2000,\n",
    "        SVM_KERNEL='linear',\n",
    "        SVM_GAMMA='auto',\n",
    "\n",
    "        # xgboost\n",
    "        XGB=False,\n",
    "        XGB_LEARNRATE=0.1,\n",
    "        XGB_MAXDEPTH=3,\n",
    "        XGB_MINCHILDWEIGHT=1,\n",
    "        XGB_COLSAMPLEBYTREE=1,\n",
    "\n",
    "        # SGD Logistic regression\n",
    "        SGD=False,\n",
    "        SGD_LOSS='log',\n",
    "        SGD_ALPHA=0.0001,\n",
    "        SGD_PENALTY='l2',\n",
    "        SGD_EPOCHS=10,\n",
    "        SGD_BATCH_SIZE=128,\n",
    "\n",
    "        # memory network\n",
    "        MEM_NET=False,\n",
    "        # The memory network vocab uses Glove which can be 50, 100, 200 or 300 depending on the models you have in /data/glove\n",
    "        MEM_VOCAB=50,\n",
    "        MEM_TYPE='dmn_basic',\n",
    "        MEM_BATCH=1,\n",
    "        MEM_EPOCHS=5,\n",
    "        MEM_MASK_MODE='sentence',\n",
    "        MEM_EMBED_MODE=\"word2vec\",\n",
    "        MEM_ONEHOT_MIN_LEN=140,\n",
    "        MEM_ONEHOT_MAX_LEN=1000,\n",
    "\n",
    "        # PARAMETERS\n",
    "        # resampling\n",
    "        RESAMPLING=False,\n",
    "        NOVEL_RATIO=None,\n",
    "        OVERSAMPLING=False,\n",
    "        REPLACEMENT=False,\n",
    "        SAVE_RESULTS=False,\n",
    "\n",
    "        # save training data for experimentation and hyperparameter grid search\n",
    "        SAVEEXPERIMENTDATA=False,\n",
    "        EXPERIMENTDATAFILE='data/experimentdatafile.pkl',\n",
    "\n",
    "        # vocabulary\n",
    "        VOCAB_SIZE=10000,\n",
    "        STEM=False,\n",
    "        FULL_VOCAB_SIZE=10000,\n",
    "        FULL_VOCAB_TYPE='character',\n",
    "        FULL_CHAR_VOCAB=\"abcdefghijklmnopqrstuvwxyz0123456789,;.!?:'\\\"/|_@#$%^&*~`+-=<>()[]{}\",\n",
    "        FULL_VOCAB_STEM=False,\n",
    "        SEED=41,\n",
    "\n",
    "        HDF5_PATH_TRAIN=None,\n",
    "        HDF5_PATH_TEST=None,\n",
    "        HDF5_SAVE_FREQUENCY=100,\n",
    "        HDF5_USE_EXISTING=True,\n",
    "\n",
    "        USE_CACHE=False):\n",
    "    \"\"\" Return a parameters data structure with information on how to\n",
    "    run an experiment. Argument list should match experiments/experiments.py\n",
    "    \"\"\"\n",
    "\n",
    "    # get features\n",
    "    bow = None\n",
    "    st = None\n",
    "    lda = None\n",
    "    w2v = None\n",
    "    wordonehot = None\n",
    "    cnn = None\n",
    "    mem_net = None\n",
    "\n",
    "    if BOW_APPEND or BOW_DIFFERENCE or BOW_PRODUCT or BOW_COS or BOW_TFIDF:\n",
    "        bow = dict()\n",
    "        if BOW_APPEND: bow['append'] = BOW_APPEND\n",
    "        if BOW_DIFFERENCE: bow['difference'] = BOW_DIFFERENCE\n",
    "        if BOW_PRODUCT: bow['product'] = BOW_PRODUCT\n",
    "        if BOW_COS: bow['cos'] = BOW_COS\n",
    "        if BOW_TFIDF: bow['tfidf'] = BOW_TFIDF\n",
    "        if BOW_BINARY: bow['binary'] = BOW_BINARY\n",
    "    if ST_APPEND or ST_DIFFERENCE or ST_PRODUCT or ST_COS:\n",
    "        st = dict()\n",
    "        if ST_APPEND: st['append'] = ST_APPEND\n",
    "        if ST_DIFFERENCE: st['difference'] = ST_DIFFERENCE\n",
    "        if ST_PRODUCT: st['product'] = ST_PRODUCT\n",
    "        if ST_COS: st['cos'] = ST_COS\n",
    "    if LDA_APPEND or LDA_DIFFERENCE or LDA_PRODUCT or LDA_COS:\n",
    "        lda = dict()\n",
    "        if LDA_APPEND: lda['append'] = LDA_APPEND\n",
    "        if LDA_DIFFERENCE: lda['difference'] = LDA_DIFFERENCE\n",
    "        if LDA_PRODUCT: lda['product'] = LDA_PRODUCT\n",
    "        if LDA_COS: lda['cos'] = LDA_COS\n",
    "        if LDA_TOPICS: lda['topics'] = LDA_TOPICS\n",
    "    if any([W2V_APPEND, W2V_DIFFERENCE, W2V_PRODUCT, W2V_COS]) or any([W2V_AVG, W2V_MAX, W2V_MIN, W2V_ABS]):\n",
    "        w2v = dict()\n",
    "        if W2V_AVG: w2v['avg'] = W2V_AVG\n",
    "        if W2V_MAX: w2v['max'] = W2V_MAX\n",
    "        if W2V_MIN: w2v['min'] = W2V_MIN\n",
    "        if W2V_ABS: w2v['abs'] = W2V_ABS\n",
    "        if W2V_APPEND: w2v['append'] = W2V_APPEND\n",
    "        if W2V_DIFFERENCE: w2v['difference'] = W2V_DIFFERENCE\n",
    "        if W2V_PRODUCT: w2v['product'] = W2V_PRODUCT\n",
    "        if W2V_COS: w2v['cos'] = W2V_COS\n",
    "        if W2V_PRETRAINED: w2v['pretrained'] = W2V_PRETRAINED\n",
    "        if W2V_MIN_COUNT: w2v['min_count'] = W2V_MIN_COUNT\n",
    "        if W2V_WINDOW: w2v['window'] = W2V_WINDOW\n",
    "        if W2V_SIZE: w2v['size'] = W2V_SIZE\n",
    "        if W2V_WORKERS: w2v['workers'] = W2V_WORKERS\n",
    "    if WORDONEHOT:\n",
    "        wordonehot = dict()\n",
    "        if WORDONEHOT_VOCAB:\n",
    "            wordonehot['vocab'] = WORDONEHOT_VOCAB\n",
    "    if CNN_APPEND or CNN_DIFFERENCE or CNN_PRODUCT or CNN_COS:\n",
    "        cnn = dict()\n",
    "        if CNN_APPEND: cnn['append'] = CNN_APPEND\n",
    "        if CNN_DIFFERENCE: cnn['difference'] = CNN_DIFFERENCE\n",
    "        if CNN_PRODUCT: cnn['product'] = CNN_PRODUCT\n",
    "        if CNN_COS: cnn['cos'] = CNN_COS\n",
    "    if MEM_NET:\n",
    "        mem_net = dict()\n",
    "        if MEM_VOCAB: mem_net['word_vector_size'] = MEM_VOCAB\n",
    "        # if SEED: mem_net['seed'] = SEED\n",
    "        if MEM_TYPE: mem_net['network'] = MEM_TYPE\n",
    "        if MEM_BATCH: mem_net['batch_size'] = MEM_BATCH\n",
    "        if MEM_EPOCHS: mem_net['epochs'] = MEM_EPOCHS\n",
    "        if MEM_MASK_MODE: mem_net['mask_mode'] = MEM_MASK_MODE\n",
    "        if MEM_EMBED_MODE: mem_net['embed_mode'] = MEM_EMBED_MODE\n",
    "        if MEM_ONEHOT_MIN_LEN: mem_net['onehot_min_len'] = MEM_ONEHOT_MIN_LEN\n",
    "        if MEM_ONEHOT_MAX_LEN: mem_net['onehot_max_len'] = MEM_ONEHOT_MAX_LEN\n",
    "        # Use the same input params as word2vec\n",
    "        if W2V_PRETRAINED: mem_net['pretrained'] = W2V_PRETRAINED\n",
    "        if W2V_MIN_COUNT: mem_net['min_count'] = W2V_MIN_COUNT\n",
    "        if W2V_WINDOW: mem_net['window'] = W2V_WINDOW\n",
    "        if W2V_SIZE: mem_net['size'] = W2V_SIZE\n",
    "        if W2V_WORKERS: mem_net['workers'] = W2V_WORKERS\n",
    "\n",
    "    features = dict()\n",
    "    if bow:\n",
    "        features['bow'] = bow\n",
    "    if st:\n",
    "        features['st'] = st\n",
    "    if lda:\n",
    "        features['lda'] = lda\n",
    "    if w2v:\n",
    "        features['w2v'] = w2v\n",
    "    if wordonehot:\n",
    "        features['wordonehot'] = wordonehot\n",
    "    if cnn:\n",
    "        features['cnn'] = cnn\n",
    "    if mem_net:\n",
    "        if len(features) > 0:\n",
    "            print(\"Caution!!  Only the memory network feature and algorithm will be ran as they have to run alone\")\n",
    "        features['mem_net'] = mem_net\n",
    "\n",
    "    if len(features) == 0:\n",
    "        print(\"Error: At least one feature (ex: Bag of Words, LDA, etc.) must be requested per run.\", file=sys.stderr)\n",
    "        quit()\n",
    "    w2v_types = [W2V_AVG, W2V_MAX, W2V_MIN, W2V_ABS]\n",
    "    w2v_ops = [W2V_APPEND, W2V_DIFFERENCE, W2V_PRODUCT, W2V_COS]\n",
    "    if any(w2v_ops) and not any(w2v_types):\n",
    "        print(\"Caution!!  A Word2Vec vector type must be selected. Default will be set to average (W2V_AVG)\",\n",
    "              file=sys.stderr)\n",
    "        features['w2v']['avg'] = True\n",
    "    if any(w2v_types) and not any(w2v_ops):\n",
    "        print(\"Caution!!  A Word2Vec vector operation must be selected. Default will be set to append (W2V_APPEND)\",\n",
    "              file=sys.stderr)\n",
    "        features['w2v']['append'] = True\n",
    "\n",
    "    # get algorithms\n",
    "    log_reg = None\n",
    "    svm = None\n",
    "    xgb = None\n",
    "    sgd = None\n",
    "\n",
    "    if LOG_REG:\n",
    "        log_reg = dict()\n",
    "        if LOG_PENALTY: log_reg['log_penalty'] = LOG_PENALTY\n",
    "        if LOG_TOL: log_reg['log_tol'] = LOG_TOL\n",
    "        if LOG_C: log_reg['log_C'] = LOG_C\n",
    "    if SVM:\n",
    "        svm = dict()\n",
    "        if SVM_C: svm['svm_C'] = SVM_C\n",
    "        if SVM_KERNEL: svm['svm_kernel'] = SVM_KERNEL\n",
    "        if SVM_GAMMA: svm['svm_gamma'] = SVM_GAMMA\n",
    "    if XGB:\n",
    "        xgb = dict()\n",
    "        if XGB_LEARNRATE: xgb['x_learning_rate'] = XGB_LEARNRATE\n",
    "        if XGB_MAXDEPTH: xgb['x_max_depth'] = XGB_MAXDEPTH\n",
    "        if XGB_COLSAMPLEBYTREE: xgb['x_colsample_bytree'] = XGB_COLSAMPLEBYTREE\n",
    "        if XGB_MINCHILDWEIGHT: xgb['x_colsample_bylevel'] = XGB_MINCHILDWEIGHT\n",
    "    if SGD:\n",
    "        sgd = dict()\n",
    "        sgd['alpha'] = SGD_ALPHA\n",
    "        sgd['loss'] = SGD_LOSS\n",
    "        sgd['penalty'] = SGD_PENALTY\n",
    "        sgd['num_epochs'] = SGD_EPOCHS\n",
    "        sgd['batch_size'] = SGD_BATCH_SIZE\n",
    "        sgd['seed'] = SEED\n",
    "        assert HDF5_PATH_TRAIN is not None, \"SGD-based methods should be used with HDF5\"\n",
    "\n",
    "    algorithms = dict()\n",
    "    if log_reg: algorithms['log_reg'] = log_reg\n",
    "    if svm: algorithms['svm'] = svm\n",
    "    if xgb: algorithms['xgb'] = xgb\n",
    "    if mem_net:\n",
    "        algorithms['mem_net'] = mem_net\n",
    "    if sgd:\n",
    "        algorithms['sgd'] = sgd\n",
    "\n",
    "    logger.debug(\"Algorithms structure: {}\".format(algorithms))\n",
    "\n",
    "    # Enforce requirement and limitation of one algorithm per run\n",
    "    if len(algorithms) == 0:\n",
    "        print(\"Error: One classification algorithm must be requested per run.\", file=sys.stderr)\n",
    "        quit()\n",
    "    elif len(algorithms) > 1:\n",
    "        print(\"Error: Only one classification can be requested per run.\", file=sys.stderr)\n",
    "        quit()\n",
    "\n",
    "    # get parameters\n",
    "    resampling = None\n",
    "\n",
    "    if RESAMPLING:\n",
    "        resampling = dict()\n",
    "        if NOVEL_RATIO:\n",
    "            resampling['novelToNotNovelRatio'] = NOVEL_RATIO\n",
    "            logger.warning(\"NOVEL_RATIO specified but not supported\")\n",
    "        resampling['over'] = OVERSAMPLING\n",
    "        resampling['replacement'] = REPLACEMENT\n",
    "\n",
    "    saveexperimentdata = None\n",
    "    if SAVEEXPERIMENTDATA:\n",
    "        saveexperimentdata = dict()\n",
    "        if EXPERIMENTDATAFILE: saveexperimentdata['experimentdatafile'] = EXPERIMENTDATAFILE\n",
    "\n",
    "    parameters = dict()\n",
    "    if RESAMPLING: parameters['resampling'] = resampling\n",
    "    if SAVE_RESULTS: parameters['save_results'] = SAVE_RESULTS\n",
    "    if SAVEEXPERIMENTDATA: parameters['saveexperimentdata'] = saveexperimentdata\n",
    "    if VOCAB_SIZE: parameters['vocab'] = VOCAB_SIZE\n",
    "    if STEM: parameters['stem'] = STEM\n",
    "    if SEED:\n",
    "        parameters['seed'] = SEED\n",
    "    else:\n",
    "        parameters['seed'] = 41\n",
    "    if FULL_VOCAB_SIZE: parameters['full_vocab_size'] = FULL_VOCAB_SIZE\n",
    "    if FULL_VOCAB_TYPE: parameters['full_vocab_type'] = FULL_VOCAB_TYPE\n",
    "    if FULL_CHAR_VOCAB: parameters['full_char_vocab'] = FULL_CHAR_VOCAB\n",
    "    if FULL_VOCAB_STEM: parameters['full_vocab_stem'] = FULL_VOCAB_STEM\n",
    "\n",
    "    assert (HDF5_PATH_TRAIN and SGD) or (not HDF5_PATH_TRAIN and not SGD)\n",
    "    parameters['hdf5_path_test'] = HDF5_PATH_TEST\n",
    "    parameters['hdf5_path_train'] = HDF5_PATH_TRAIN\n",
    "    parameters['hdf5_save_frequency'] = HDF5_SAVE_FREQUENCY\n",
    "    parameters['hdf5_use_existing'] = HDF5_USE_EXISTING\n",
    "    parameters['use_cache'] = USE_CACHE\n",
    "\n",
    "    return directory, features, algorithms, parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26731de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(argv):\n",
    "    \"\"\"\n",
    "    controls the over-arching implmentation of the algorithms\n",
    "    \"\"\"\n",
    "    print('starting')\n",
    "    directory, features, algorithms, parameters = argv\n",
    "\n",
    "    # Create a numpy random state\n",
    "    random_state = np.random.RandomState(parameters['seed'])\n",
    "\n",
    "    # parsing\n",
    "    print(\"parsing json data...\", file=sys.stderr)\n",
    "\n",
    "    if parameters['use_cache']:\n",
    "        hash = dir_hash(directory)\n",
    "        pickle_path = os.path.join(cache_dir, cache_pickle.format(hash))\n",
    "        try:\n",
    "            logger.debug(\"Trying to use cache\")\n",
    "            with open(pickle_path, 'rb') as f:\n",
    "                parsed_data = pickle.load(f)\n",
    "                logger.debug(\"Using existing cache\")\n",
    "        except:\n",
    "            # parse and write to cache\n",
    "            logger.debug(\"Parsing and writing to cache\")\n",
    "            parsed_data = parse_json(directory, parameters)\n",
    "            os.makedirs(cache_dir, exist_ok=True)\n",
    "            with open(pickle_path, 'wb') as f:\n",
    "                pickle.dump(parsed_data, f)\n",
    "    else:\n",
    "        parsed_data = parse_json(directory, parameters)\n",
    "    clusters, order, data, test_clusters, test_order, test_data, corpusdict = parsed_data\n",
    "\n",
    "    # preprocessing\n",
    "    print(\"preprocessing...\", file=sys.stderr)\n",
    "    vocab, full_vocab, encoder_decoder, lda_model, tf_model, w2v_model = preprocess(features, parameters,\n",
    "                                                                                         corpusdict, data)\n",
    "\n",
    "    # featurization\n",
    "    hdf5_path_train = parameters['hdf5_path_train']\n",
    "    hdf5_path_test = parameters['hdf5_path_test']\n",
    "    print(\"generating training data...\", file=sys.stderr)\n",
    "    train_data, train_target, train_ids = gen_observations(clusters, order, data, features, parameters, vocab,\n",
    "                                                                    full_vocab, encoder_decoder, lda_model, tf_model,\n",
    "                                                                    w2v_model, hdf5_path_train)\n",
    "    print(\"generating testing data...\", file=sys.stderr)\n",
    "    test_data, test_target, test_ids = gen_observations(test_clusters, test_order, test_data, features,\n",
    "                                                                 parameters, vocab, full_vocab, encoder_decoder,\n",
    "                                                                 lda_model, tf_model, w2v_model, hdf5_path_test)\n",
    "\n",
    "    # save training data for separate experimentation and hyperparameter optimization\n",
    "    if 'saveexperimentdata' in parameters:\n",
    "        lunchbox = dict()\n",
    "        lunchbox['directory'] = directory\n",
    "        lunchbox['features'] = features\n",
    "        lunchbox['algorithms'] = algorithms\n",
    "        lunchbox['parameters'] = parameters\n",
    "        lunchbox['train_data'] = train_data\n",
    "        lunchbox['train_target'] = train_target\n",
    "        lunchbox['test_data'] = test_data\n",
    "        lunchbox['test_target'] = test_target\n",
    "        pickle.dump(lunchbox, open(parameters['saveexperimentdata']['experimentdatafile'], \"wb\"))\n",
    "\n",
    "    # modeling\n",
    "    print(\"running algorithms...\", file=sys.stderr)\n",
    "    if 'mem_net' in algorithms:\n",
    "        mem_net_model, model_name = run_mem_net(train_data, test_data, **algorithms['mem_net'])\n",
    "        predicted_labels, perform_results = test_mem_network(mem_net_model, model_name,\n",
    "                                                                          **algorithms['mem_net'])\n",
    "    # results\n",
    "    if \"save_results\" in parameters:\n",
    "        perform_results.update({\"id\": test_ids})\n",
    "        perform_results.update({\"predicted_label\": predicted_labels.tolist()})\n",
    "        perform_results.update({\"novelty\": test_target})\n",
    "\n",
    "    return perform_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33bebde2",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    args = get_args()\n",
    "    print(\"Algorithm details and Results:\", file=sys.stderr)\n",
    "    print(main(args), file=sys.stdout)\n",
    "    sys.exit(0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
